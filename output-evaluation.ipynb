{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93307a99-4fb6-4883-b204-8fe55d17c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: TER results written to caesar-cipher-benchmark-result.csv\n",
      "INFO: Token-based performance results written to Token_Based_Performance_Result.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Directory containing the JSON files\n",
    "        self.data_directory = './data/encoded/caesar-cipher/'\n",
    "        # Path to the configuration file\n",
    "        self.config_file_path = './caesar-cipher-experiments-log.txt'\n",
    "        # Output file names\n",
    "        self.ter_csv_file = './eval/caesar_cipher/caesar-cipher-benchmark-result.csv'\n",
    "        self.token_csv_file = './eval/caesar_cipher/token_based_performance_result.csv'\n",
    "        # You can add more configuration parameters here\n",
    "\n",
    "class CaesarCipherTest:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.directory = self.config.data_directory\n",
    "        self.config_file = self.config.config_file_path\n",
    "        self.config_data = self.parse_config_file()\n",
    "        self.global_token_accumulated = defaultdict(lambda: {'accuracy_sum': 0, 'count': 0})\n",
    "        self.ter_results = []\n",
    "\n",
    "    def parse_config_file(self) -> Dict[str, Dict[str, str]]:\n",
    "        config_data = {}\n",
    "        try:\n",
    "            with open(self.config_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        timestamp = self.extract_timestamp(line)\n",
    "                        fields = self.parse_filename(line)\n",
    "                        config_data[timestamp] = fields\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Config file {self.config_file} not found.\")\n",
    "        return config_data\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_timestamp(filename: str) -> str:\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0]}_{parts[1]}\"\n",
    "        return parts[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_filename(filename: str) -> Dict[str, str]:\n",
    "        # Remove the .json extension if any\n",
    "        if filename.endswith('.json'):\n",
    "            filename = filename[:-5]\n",
    "\n",
    "        # Split filename into parts\n",
    "        parts = filename.split('__')\n",
    "        pre_model_info = parts[0]\n",
    "        post_model_info = parts[1] if len(parts) > 1 else ''\n",
    "        pre_fields = pre_model_info.split('_')\n",
    "\n",
    "        # Extract fields\n",
    "        date = pre_fields[0]\n",
    "        time = pre_fields[1]\n",
    "        samples = pre_fields[2]\n",
    "        shift = pre_fields[3]\n",
    "        prompt_type = '_'.join(pre_fields[4:6])\n",
    "        method = pre_fields[6]\n",
    "        shot_and_model_name_parts = pre_fields[7:]\n",
    "        shot_and_model_name = '_'.join(shot_and_model_name_parts)\n",
    "\n",
    "        # Handle shot and model_name\n",
    "        if '-' in shot_and_model_name:\n",
    "            shot, model_name = shot_and_model_name.split('-', 1)\n",
    "        else:\n",
    "            shot = ''\n",
    "            model_name = shot_and_model_name\n",
    "\n",
    "        # Handle 'models--' prefix in model_name\n",
    "        if model_name.startswith('models--'):\n",
    "            model_name = model_name[len('models--'):]\n",
    "            model_name_parts = model_name.split('--')\n",
    "            model_name = model_name_parts[-1]\n",
    "\n",
    "        # Handle temperature and max_token\n",
    "        if post_model_info:\n",
    "            post_fields = post_model_info.split('_')\n",
    "            temperature = post_fields[0]\n",
    "            max_token = post_fields[1] if len(post_fields) > 1 else ''\n",
    "        else:\n",
    "            temperature = ''\n",
    "            max_token = ''\n",
    "\n",
    "        return {\n",
    "            'date': date,\n",
    "            'time': time,\n",
    "            'samples': samples,\n",
    "            'shift': shift,\n",
    "            'prompt_type': prompt_type,\n",
    "            'method': method,\n",
    "            'shot': shot,\n",
    "            'model_name': model_name,\n",
    "            'temperature': temperature,\n",
    "            'max_token': max_token\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ter(pred: str, gold: str) -> Tuple[float, List[str], List[str]]:\n",
    "        pred_tokens = [token for token in pred.strip().split() if token]\n",
    "        gold_tokens = [token for token in gold.strip().split() if token]\n",
    "\n",
    "        # Calculate token-level errors\n",
    "        errors = sum(1 for p, g in zip(pred_tokens, gold_tokens) if p != g)\n",
    "        errors += abs(len(pred_tokens) - len(gold_tokens))\n",
    "\n",
    "        max_len = max(len(pred_tokens), len(gold_tokens))\n",
    "        error_rate = errors / max_len if max_len > 0 else 1  # Avoid division by zero\n",
    "        return error_rate, pred_tokens, gold_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def process_token_based_performance(token_performance: List[Tuple[int, float, float]]) -> Dict[int, Dict[str, float]]:\n",
    "        token_accumulated = defaultdict(lambda: {'accuracy_sum': 0, 'count': 0})\n",
    "        for token_index, token_acc, _ in token_performance:\n",
    "            token_accumulated[token_index]['accuracy_sum'] += token_acc\n",
    "            token_accumulated[token_index]['count'] += 1\n",
    "        return token_accumulated\n",
    "\n",
    "    @staticmethod\n",
    "    def write_average_token_accuracy(writer, token_accumulated: Dict[int, Dict[str, float]]):\n",
    "        for token_index, data in sorted(token_accumulated.items()):\n",
    "            average_accuracy = data['accuracy_sum'] / data['count'] if data['count'] > 0 else 0\n",
    "            writer.writerow([token_index, f\"{average_accuracy:.2f}\"])\n",
    "\n",
    "    def process_json_file(self, filepath: str, fields: Dict[str, str]) -> Optional[Dict]:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    logging.warning(f\"Skipping empty file: {os.path.basename(filepath)}\")\n",
    "                    return None\n",
    "                data = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"Error decoding JSON in file: {os.path.basename(filepath)}\")\n",
    "            return None\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"File not found: {filepath}\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(data, list):\n",
    "            logging.error(f\"File {os.path.basename(filepath)} does not contain a list of records.\")\n",
    "            return None\n",
    "\n",
    "        # Initialize accumulators\n",
    "        total_accuracy = 0.0\n",
    "        total_error_rate = 0.0\n",
    "        total_correct_samples = 0\n",
    "        total_matching_tokens = 0\n",
    "        total_tokens = 0\n",
    "        record_count = 0\n",
    "        token_performance = []\n",
    "\n",
    "        for record in data:\n",
    "            cipher_text = record.get(\"cipher_text\", \"\").strip()\n",
    "            gold_label = record.get(\"gold_label\", \"\").strip()\n",
    "\n",
    "            if not cipher_text and not gold_label:\n",
    "                continue\n",
    "\n",
    "            error_rate, pred_tokens, gold_tokens = self.calculate_ter(cipher_text, gold_label)\n",
    "\n",
    "            matching_tokens = sum(1 for p, g in zip(pred_tokens, gold_tokens) if p == g)\n",
    "            total_matching_tokens += matching_tokens\n",
    "            total_tokens += max(len(pred_tokens), len(gold_tokens))\n",
    "\n",
    "            accuracy = 1 - error_rate\n",
    "            total_accuracy += accuracy\n",
    "            total_error_rate += error_rate\n",
    "            record_count += 1\n",
    "\n",
    "            if cipher_text == gold_label and cipher_text:\n",
    "                total_correct_samples += 1\n",
    "                logging.info(f\"Exact match found in file {os.path.basename(filepath)}:\")\n",
    "                logging.info(f\"cipher_text: {cipher_text}\")\n",
    "                logging.info(f\"gold_label: {gold_label}\")\n",
    "                logging.info(f\"Record: {record}\")\n",
    "\n",
    "            # Token-based performance\n",
    "            for i, (p, g) in enumerate(zip(pred_tokens, gold_tokens)):\n",
    "                if not p and not g:\n",
    "                    continue\n",
    "                token_accuracy = 1 if p == g else 0\n",
    "                token_error_rate = 1 - token_accuracy\n",
    "                token_performance.append((i + 1, token_accuracy * 100, token_error_rate * 100))\n",
    "\n",
    "        if record_count > 0:\n",
    "            average_accuracy = (total_accuracy / record_count) * 100\n",
    "            average_error_rate = (total_error_rate / record_count) * 100\n",
    "            sample_accuracy = (total_correct_samples / record_count) * 100\n",
    "            token_exact_match_accuracy = (total_matching_tokens / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "        else:\n",
    "            average_accuracy = 0.0\n",
    "            average_error_rate = 0.0\n",
    "            sample_accuracy = 0.0\n",
    "            token_exact_match_accuracy = 0.0\n",
    "\n",
    "        # Process token-based performance\n",
    "        token_accumulated = self.process_token_based_performance(token_performance)\n",
    "\n",
    "        return {\n",
    "            'filename': os.path.basename(filepath),\n",
    "            'model_name': fields.get('model_name', ''),\n",
    "            'shift': fields.get('shift', ''),\n",
    "            'prompt_type': fields.get('prompt_type', ''),\n",
    "            'temperature': fields.get('temperature', ''),\n",
    "            'max_token': fields.get('max_token', ''),\n",
    "            'average_accuracy': average_accuracy,\n",
    "            'sample_accuracy': sample_accuracy,\n",
    "            'average_error_rate': average_error_rate,\n",
    "            'token_exact_match_accuracy': token_exact_match_accuracy,\n",
    "            'token_accumulated': token_accumulated\n",
    "        }\n",
    "\n",
    "    def process_directory(self):\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(self.directory, filename)\n",
    "                filename_without_ext = filename[:-5]\n",
    "\n",
    "                timestamp = self.extract_timestamp(filename_without_ext)\n",
    "                fields = self.config_data.get(timestamp)\n",
    "                if not fields:\n",
    "                    logging.warning(f\"Timestamp {timestamp} from filename {filename_without_ext} not found in {self.config_file}\")\n",
    "                    continue\n",
    "\n",
    "                result_data = self.process_json_file(filepath, fields)\n",
    "                if result_data:\n",
    "                    self.ter_results.append(result_data)\n",
    "                    # Merge token performance into global accumulator\n",
    "                    token_accumulated = result_data['token_accumulated']\n",
    "                    for token_index, data in token_accumulated.items():\n",
    "                        self.global_token_accumulated[token_index]['accuracy_sum'] += data['accuracy_sum']\n",
    "                        self.global_token_accumulated[token_index]['count'] += data['count']\n",
    "\n",
    "        # Sort results\n",
    "        self.ter_results.sort(key=lambda x: (x['model_name'], int(x['shift']), x['prompt_type']))\n",
    "\n",
    "    def write_results(self):\n",
    "        # Write TER results\n",
    "        ter_csv_file = self.config.ter_csv_file\n",
    "        with open(ter_csv_file, mode='w', newline='', encoding='utf-8') as ter_file:\n",
    "            ter_writer = csv.writer(ter_file)\n",
    "            ter_writer.writerow([\n",
    "                'Filename', 'Model', 'Shift', 'Prompt Type', 'Temperature', 'Max Token',\n",
    "                'Average Accuracy (%)', 'Sample-level Accuracy (%)', 'Average Error Rate (%)',\n",
    "                'Token Exact Match Accuracy (%)'\n",
    "            ])\n",
    "            for result in self.ter_results:\n",
    "                ter_writer.writerow([\n",
    "                    result['filename'],\n",
    "                    result['model_name'],\n",
    "                    result['shift'],\n",
    "                    result['prompt_type'],\n",
    "                    result['temperature'],\n",
    "                    result['max_token'],\n",
    "                    f\"{result['average_accuracy']:.2f}\",\n",
    "                    f\"{result['sample_accuracy']:.2f}\",\n",
    "                    f\"{result['average_error_rate']:.2f}\",\n",
    "                    f\"{result['token_exact_match_accuracy']:.2f}\"\n",
    "                ])\n",
    "        logging.info(f\"TER results written to {ter_csv_file}\")\n",
    "\n",
    "        # Write token-based performance\n",
    "        token_csv_file = self.config.token_csv_file\n",
    "        with open(token_csv_file, mode='w', newline='', encoding='utf-8') as token_file:\n",
    "            token_writer = csv.writer(token_file)\n",
    "            token_writer.writerow(['Token Index', 'Average Accuracy (%)'])\n",
    "            self.write_average_token_accuracy(token_writer, self.global_token_accumulated)\n",
    "        logging.info(f\"Token-based performance results written to {token_csv_file}\")\n",
    "\n",
    "    def run(self):\n",
    "        self.process_directory()\n",
    "        self.write_results()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    # Create an instance of CaesarCipherTest with the configuration\n",
    "    caesar_test = CaesarCipherTest(config)\n",
    "    caesar_test.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
