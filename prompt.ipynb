{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c075baa-9960-427e-b01f-cc4b529d7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip3 install bitsandbytes\n",
    "# !pip3 install --upgrade accelerate\n",
    "# !pip install transformers[sentencepiece]\n",
    "\n",
    "# import torch\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e56c1-4a5c-4596-8d8e-e22be1a3ae6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2be3fa59384319b4baebcc112d66c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting encoding with model models--mistralai--Mixtral-8x7B-Instruct-v0.1 using prompt type 'cot-like' (Model 1 of 1) with shift 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress with models--mistralai--Mixtral-8x7B-Instruct-v0.1 (cot-like):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished encoding with models--mistralai--Mixtral-8x7B-Instruct-v0.1 using prompt type 'cot-like' and shift 12. Results saved to ./data/encoded/caesar-cipher/20241202_090613.json\n",
      "\n",
      "\n",
      "Starting encoding with model models--mistralai--Mixtral-8x7B-Instruct-v0.1 using prompt type 'default' (Model 1 of 1) with shift 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress with models--mistralai--Mixtral-8x7B-Instruct-v0.1 (default): "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished encoding with models--mistralai--Mixtral-8x7B-Instruct-v0.1 using prompt type 'default' and shift 12. Results saved to ./data/encoded/caesar-cipher/20241202_100317.json\n",
      "\n",
      "Freed up GPU memory after processing models--mistralai--Mixtral-8x7B-Instruct-v0.1. Moving to the next model.\n",
      "\n",
      "All models, prompt types, and shift values have been processed. Exiting.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datetime import datetime\n",
    "\n",
    "# =======================\n",
    "# Configuration Section\n",
    "# =======================\n",
    "CONFIG = {\n",
    "    # File paths and directories\n",
    "    'PATHS': {\n",
    "        'INPUT_FILE': './data/original/natural.txt',\n",
    "        'OUTPUT_DIR': './data/encoded/caesar-cipher/',\n",
    "        'CONFIG_FILE': './log/caesar-cipher-experiments-log.txt'\n",
    "    },\n",
    "    \n",
    "    # Model configurations\n",
    "    'MODELS': [\n",
    "        '/storage/ukp/shared/shared_model_weights/models--llama-3/8B-Instruct/',\n",
    "        '/storage/ukp/shared/shared_model_weights/models--llama-3/70B-Instruct/',\n",
    "        '/storage/ukp/shared/shared_model_weights/models--mistralai--Mistral-7B-Instruct-v0.3/',\n",
    "        '/storage/ukp/shared/shared_model_weights/models--mistralai--Mixtral-8x7B-Instruct-v0.1/'\n",
    "    ],\n",
    "    \n",
    "    # Experiment settings\n",
    "    'EXPERIMENT': {\n",
    "        'TOTAL_LINES': 25,\n",
    "        'SHIFT_VALUES': [12],\n",
    "        'TEMPERATURE': 0.01,\n",
    "        'SEED': 2266\n",
    "    },\n",
    "    \n",
    "    # Model generation settings\n",
    "    'GENERATION': {\n",
    "        'MAX_TOKENS': {\n",
    "            'base': 64,\n",
    "            'cot-like': 512,\n",
    "            'default': 2024,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Prompt templates\n",
    "    'PROMPTS': {\n",
    "        'base': \"Encode the following text to a Caesar cipher. The shift is $SHIFT$. Output the cipher text only.\\n\\nplain text: \",\n",
    "        'cot-like': \"Encode the following text to a Caesar cipher. The shift is $SHIFT$. Output a lookup table and the cipher text in a Python dictionary: {\\\"lookup_table\\\": {}, \\\"cipher_text\\\":...}. Output the dictionary only.\\n\\nplain text: \",\n",
    "        'default': \"Encode the following text to a Caesar cipher. The shift is $SHIFT$.\\n\\nplain text: \"\n",
    "    },\n",
    "    \n",
    "    # Regex patterns for text extraction\n",
    "    'PATTERNS': {\n",
    "        'LOOKUP_TABLE': r\"(\\\"lookup_table\\\":|\\'lookup_table\\':)\\s*(\\{[^\\}]*\\})\",\n",
    "        'CIPHER_TEXT': r\"(\\\"cipher_text\\\":|\\'cipher_text\\':|`cipher_text`:)\\s*(\\\"[^\\\"]*\\\"|'[^']*'|`[^`]*`)\",\n",
    "        'GENERAL_CIPHER': r\"(Cipher text:|cipher text:|cipher_text:|shifted text:|Caesar cipher:|The encoded text is:|encoded text:|cipher text is:|answer is|Answer:|Answer is:|encoded text is|output|Solution:)(?!\\s*\\?:?$)\\s*((?!plain text|encoded text).+)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Initialization\n",
    "# =======================\n",
    "def setup_environment():\n",
    "    \"\"\"Initialize environment settings and suppress warnings\"\"\"\n",
    "    random.seed(CONFIG['EXPERIMENT']['SEED'])\n",
    "    torch.manual_seed(CONFIG['EXPERIMENT']['SEED'])\n",
    "    torch.cuda.manual_seed_all(CONFIG['EXPERIMENT']['SEED'])\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    warnings.filterwarnings(\"ignore\", message=\"huggingface/tokenizers: The current process just got forked\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# =======================\n",
    "# Encoder Class Definition\n",
    "# =======================\n",
    "class TextEncoder:\n",
    "    def __init__(self, model_path, temperature=0.01):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.temperature = temperature\n",
    "        self.model_name = self._get_model_name(model_path)\n",
    "    \n",
    "    def _get_model_name(self, model_path):\n",
    "        \"\"\"Extract model name from path\"\"\"\n",
    "        return model_path.split('models--')[-1].replace('/', '_') if \"llama\" in model_path else model_path.split('/')[-2]\n",
    "\n",
    "    def encode(self, prompt_template, text, shift, max_tokens, prompt_type):\n",
    "        prompt = prompt_template.replace(\"$SHIFT$\", str(shift)) + text\n",
    "        return self.generate_text(prompt, max_tokens, prompt_type)\n",
    "\n",
    "    def generate_text(self, prompt, max_tokens, prompt_type):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        generated_text = \"\\n\".join([line for line in generated_text.splitlines() if line.strip()]).replace(\"`\", \"\")\n",
    "\n",
    "        #print(f\"GENERATED_TEXT: {generated_text}\")\n",
    "        \n",
    "        return self._process_generated_text(generated_text, prompt_type)\n",
    "    \n",
    "    def _process_generated_text(self, generated_text, prompt_type):\n",
    "        \"\"\"Process the generated text based on prompt type\"\"\"\n",
    "        if prompt_type == 'cot-like':\n",
    "            return self._process_cot_like_text(generated_text)\n",
    "        return self._process_regular_text(generated_text)\n",
    "    \n",
    "    def _process_cot_like_text(self, generated_text):\n",
    "        \"\"\"Process text for cot-like prompt type\"\"\"\n",
    "        lookup_table_matches = re.findall(CONFIG['PATTERNS']['LOOKUP_TABLE'], generated_text)\n",
    "        cipher_text_matches = re.findall(CONFIG['PATTERNS']['CIPHER_TEXT'], generated_text)\n",
    "        \n",
    "        final_lookup_table = next((match[1] for match in lookup_table_matches if match[1] and len(match[1]) > 2), None)\n",
    "        final_cipher_text = next((match[1].strip(\"\\\"'`\") for match in cipher_text_matches if match[1] and len(match[1].strip()) > 0), \"\")\n",
    "        \n",
    "        try:\n",
    "            parsed_lookup_table = json.loads(final_lookup_table) if final_lookup_table else {}\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_lookup_table = final_lookup_table if final_lookup_table else {}\n",
    "            \n",
    "        return parsed_lookup_table, final_cipher_text\n",
    "    \n",
    "    def _process_regular_text(self, generated_text):\n",
    "        \"\"\"Process text for regular prompt types\"\"\"\n",
    "        matches = re.findall(CONFIG['PATTERNS']['GENERAL_CIPHER'], generated_text)\n",
    "        # print(f\"MATCHES: {matches}\")\n",
    "        \n",
    "        cipher_texts = [match[-1].strip() for match in matches if match[-1].strip()]\n",
    "        \n",
    "        if not cipher_texts:\n",
    "            fallback_match = re.findall(CONFIG['PATTERNS']['GENERAL_CIPHER'], generated_text)\n",
    "            # print(f\"FALLBACK_MATCH: {fallback_match}\")\n",
    "            if fallback_match:\n",
    "                cipher_texts.append(fallback_match[0][-1].strip())\n",
    "\n",
    "        if len(cipher_texts)>3:\n",
    "            cipher_texts=cipher_texts[:-1]\n",
    "        \n",
    "        return {}, cipher_texts[-1] if cipher_texts else generated_text.strip()\n",
    "\n",
    "def save_results(entry, json_file, is_last):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    json.dump(entry, json_file, indent=4)\n",
    "    json_file.write(\"\\n\" if is_last else \",\\n\")\n",
    "\n",
    "def write_config_line(experiment_id, total_lines, shift, type_of_plain_text, prompt_type, model, temperature, max_tokens):\n",
    "    \"\"\"Write configuration to config file\"\"\"\n",
    "    config_line = f\"{experiment_id}_{total_lines}_{shift}_{type_of_plain_text}_{prompt_type}_encrypt_0-{model}_{temperature}_{max_tokens}\\n\"\n",
    "    with open(CONFIG['PATHS']['CONFIG_FILE'], 'a') as config_file:\n",
    "        config_file.write(config_line)\n",
    "\n",
    "def process_model(model_path, lines_to_encode, shift, type_of_plain_text, model_index, total_models):\n",
    "    \"\"\"Process a single model\"\"\"\n",
    "    encoder = TextEncoder(model_path, CONFIG['EXPERIMENT']['TEMPERATURE'])\n",
    "    \n",
    "    for prompt_type, prompt_template in CONFIG['PROMPTS'].items():\n",
    "        max_tokens = CONFIG['GENERATION']['MAX_TOKENS'][prompt_type]\n",
    "        print(f\"\\nStarting encoding with model {encoder.model_name} using prompt type '{prompt_type}' (Model {model_index + 1} of {total_models}) with shift {shift}\")\n",
    "        \n",
    "        experiment_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file_path = os.path.join(CONFIG['PATHS']['OUTPUT_DIR'], f\"{experiment_id}.json\")\n",
    "        \n",
    "        write_config_line(experiment_id, len(lines_to_encode), shift, type_of_plain_text, \n",
    "                         prompt_type, encoder.model_name, encoder.temperature, max_tokens)\n",
    "        \n",
    "        with open(output_file_path, 'w') as json_file:\n",
    "            json_file.write(\"[\\n\")\n",
    "            \n",
    "            for line_idx, line in enumerate(tqdm(lines_to_encode, desc=f\"Encoding Progress with {encoder.model_name} ({prompt_type})\", dynamic_ncols=True)):\n",
    "                plain_text = line.strip()\n",
    "                if plain_text:\n",
    "                    lookup_table, cipher_text = encoder.encode(prompt_template, plain_text, shift, max_tokens, prompt_type)\n",
    "                    # print(f\"CIPHER_TEXT: {cipher_text}\")\n",
    "                    \n",
    "                    entry = {\n",
    "                        'plain_text': plain_text,\n",
    "                        'shift': shift,\n",
    "                        'cipher_text': cipher_text\n",
    "                    }\n",
    "                    if prompt_type == 'cot-like':\n",
    "                        entry['lookup_table'] = lookup_table\n",
    "                    \n",
    "                    save_results(entry, json_file, line_idx == len(lines_to_encode) - 1)\n",
    "            \n",
    "            json_file.write(\"]\\n\")\n",
    "        \n",
    "        print(f\"Finished encoding with {encoder.model_name} using prompt type '{prompt_type}' and shift {shift}. Results saved to {output_file_path}\\n\")\n",
    "    \n",
    "    # Clean up\n",
    "    del encoder.model\n",
    "    del encoder.tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Freed up GPU memory after processing {encoder.model_name}. Moving to the next model.\\n\")\n",
    "\n",
    "def main():\n",
    "    setup_environment()\n",
    "    \n",
    "    # Read input file\n",
    "    with open(CONFIG['PATHS']['INPUT_FILE'], 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    lines_to_encode = lines[:CONFIG['EXPERIMENT']['TOTAL_LINES']]\n",
    "    type_of_plain_text = os.path.splitext(os.path.basename(CONFIG['PATHS']['INPUT_FILE']))[0]\n",
    "    \n",
    "    # Process each shift value and model\n",
    "    for shift in CONFIG['EXPERIMENT']['SHIFT_VALUES']:\n",
    "        for model_index, model_path in enumerate(CONFIG['MODELS']):\n",
    "            process_model(model_path, lines_to_encode, shift, type_of_plain_text, \n",
    "                        model_index, len(CONFIG['MODELS']))\n",
    "    \n",
    "    print(\"All models, prompt types, and shift values have been processed. Exiting.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed01232-b47c-40d8-856d-3f3f7ee55b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
