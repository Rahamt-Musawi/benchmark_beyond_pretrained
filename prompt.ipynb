{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c075baa-9960-427e-b01f-cc4b529d7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip3 install bitsandbytes\n",
    "# !pip3 install --upgrade accelerate\n",
    "# !pip install transformers[sentencepiece]\n",
    "\n",
    "# import torch\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e56c1-4a5c-4596-8d8e-e22be1a3ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# CAESAR CIPHER - NATURAL, RANDOM, AND GREEK TEXT INPUTS\n",
    "# =======================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datetime import datetime\n",
    "\n",
    "# =======================\n",
    "# Configuration Section\n",
    "# =======================\n",
    "CONFIG = {\n",
    "    # File paths and directories\n",
    "    'PATHS': {\n",
    "        # 'INPUT_FILE': './data/original/natural.txt',\n",
    "        # 'INPUT_FILE': './data/original/random.txt',\n",
    "        'INPUT_FILE': './data/original/natural-greek.txt',\n",
    "        # 'OUTPUT_DIR': './data/encoded/caesar-cipher/natural/',\n",
    "        # 'OUTPUT_DIR': './data/encoded/caesar-cipher/random/',\n",
    "        'OUTPUT_DIR': './data/encoded/caesar-cipher/greek/',\n",
    "        # 'CONFIG_FILE': './caesar-cipher-experiments-log-natural-text.txt'\n",
    "        # 'CONFIG_FILE': './caesar-cipher-experiments-log-random-text.txt'\n",
    "        'CONFIG_FILE': './caesar-cipher-experiments-log-greek-text.txt'\n",
    "    },\n",
    "    \n",
    "    # Model configurations\n",
    "    'MODELS': [\n",
    "        # '/storage/ukp/shared/shared_model_weights/models--llama-3/8B-Instruct/',\n",
    "        # '/storage/ukp/shared/shared_model_weights/models--llama-3/70B-Instruct/',\n",
    "        '/storage/ukp/shared/shared_model_weights/models--mistralai--Mistral-7B-Instruct-v0.3/',\n",
    "        '/storage/ukp/shared/shared_model_weights/models--mistralai--Mixtral-8x7B-Instruct-v0.1/'\n",
    "    ],\n",
    "    \n",
    "    # Experiment settings\n",
    "    'EXPERIMENT': {\n",
    "        'TOTAL_LINES': 25,\n",
    "        'SHIFT_VALUES': [12],\n",
    "        'TEMPERATURE': 0.01,\n",
    "        'SEED': 2266\n",
    "    },\n",
    "    \n",
    "    # Model generation settings\n",
    "    'GENERATION': {\n",
    "        'MAX_TOKENS': {\n",
    "            'base': 64,\n",
    "            'cot-like': 512,\n",
    "            'default': 2024,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Prompt templates\n",
    "    'PROMPTS': {\n",
    "        'base': \"Encode the following text to a Caesar cipher. The shift is $SHIFT$. Output the cipher text only.\\n\\nplain text: \",\n",
    "        'cot-like': \"Encode the following text to a Caesar cipher. The shift is $SHIFT$. Output a lookup table and the cipher text in a Python dictionary: {\\\"lookup_table\\\": {}, \\\"cipher_text\\\":...}. Output the dictionary only.\\n\\nplain text: \",\n",
    "        'default': \"Encode the following text to a Caesar cipher. The shift is $SHIFT$.\\n\\nplain text: \"\n",
    "    },\n",
    "    \n",
    "    # Regex patterns for text extraction\n",
    "    'PATTERNS': {\n",
    "        'LOOKUP_TABLE': r\"(\\\"lookup_table\\\":|\\'lookup_table\\':)\\s*(\\{[^\\}]*\\})\",\n",
    "        'CIPHER_TEXT': r\"(\\\"cipher_text\\\":|\\'cipher_text\\':|`cipher_text`:)\\s*(\\\"[^\\\"]*\\\"|'[^']*'|`[^`]*`)\",\n",
    "        'GENERAL_CIPHER': r\"(Cipher text:|cipher text:|cipher_text:|shifted text:|Caesar cipher:|The encoded text is:|encoded text:|cipher text is:|answer is|Answer:|Answer is:|encoded text is|output|Solution:|The final answer is)(?!\\s*\\?:?$)\\s*((?!plain text|encoded text).+)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Initialization\n",
    "# =======================\n",
    "def setup_environment():\n",
    "    \"\"\"Initialize environment settings and suppress warnings\"\"\"\n",
    "    random.seed(CONFIG['EXPERIMENT']['SEED'])\n",
    "    torch.manual_seed(CONFIG['EXPERIMENT']['SEED'])\n",
    "    torch.cuda.manual_seed_all(CONFIG['EXPERIMENT']['SEED'])\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    warnings.filterwarnings(\"ignore\", message=\"huggingface/tokenizers: The current process just got forked\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# =======================\n",
    "# Encoder Class Definition\n",
    "# =======================\n",
    "class TextEncoder:\n",
    "    def __init__(self, model_path, temperature=0.01):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.temperature = temperature\n",
    "        self.model_name = self._get_model_name(model_path)\n",
    "    \n",
    "    def _get_model_name(self, model_path):\n",
    "        \"\"\"Extract model name from path\"\"\"\n",
    "        return model_path.split('models--')[-1].replace('/', '_') if \"llama\" in model_path else model_path.split('/')[-2]\n",
    "\n",
    "    def encode(self, prompt_template, text, shift, max_tokens, prompt_type):\n",
    "        prompt = prompt_template.replace(\"$SHIFT$\", str(shift)) + text\n",
    "        return self.generate_text(prompt, max_tokens, prompt_type)\n",
    "\n",
    "    def generate_text(self, prompt, max_tokens, prompt_type):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        generated_text = \"\\n\".join([line for line in generated_text.splitlines() if line.strip()]).replace(\"`\", \"\")\n",
    "\n",
    "        #print(f\"GENERATED_TEXT: {generated_text}\")\n",
    "        \n",
    "        return self._process_generated_text(generated_text, prompt_type)\n",
    "    \n",
    "    def _process_generated_text(self, generated_text, prompt_type):\n",
    "        \"\"\"Process the generated text based on prompt type\"\"\"\n",
    "        if prompt_type == 'cot-like':\n",
    "            return self._process_cot_like_text(generated_text)\n",
    "        return self._process_regular_text(generated_text)\n",
    "    \n",
    "    def _process_cot_like_text(self, generated_text):\n",
    "        \"\"\"Process text for cot-like prompt type\"\"\"\n",
    "        lookup_table_matches = re.findall(CONFIG['PATTERNS']['LOOKUP_TABLE'], generated_text)\n",
    "        cipher_text_matches = re.findall(CONFIG['PATTERNS']['CIPHER_TEXT'], generated_text)\n",
    "        \n",
    "        final_lookup_table = next((match[1] for match in lookup_table_matches if match[1] and len(match[1]) > 2), None)\n",
    "        final_cipher_text = next((match[1].strip(\"\\\"'`\") for match in cipher_text_matches if match[1] and len(match[1].strip()) > 0), \"\")\n",
    "        \n",
    "        try:\n",
    "            parsed_lookup_table = json.loads(final_lookup_table) if final_lookup_table else {}\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_lookup_table = final_lookup_table if final_lookup_table else {}\n",
    "            \n",
    "        return parsed_lookup_table, final_cipher_text\n",
    "    \n",
    "    def _process_regular_text(self, generated_text):\n",
    "        \"\"\"Process text for regular prompt types\"\"\"\n",
    "        matches = re.findall(CONFIG['PATTERNS']['GENERAL_CIPHER'], generated_text)\n",
    "        # print(f\"MATCHES: {matches}\")\n",
    "        \n",
    "        cipher_texts = [match[-1].strip() for match in matches if match[-1].strip()]\n",
    "        \n",
    "        if not cipher_texts:\n",
    "            fallback_match = re.findall(CONFIG['PATTERNS']['GENERAL_CIPHER'], generated_text)\n",
    "            # print(f\"FALLBACK_MATCH: {fallback_match}\")\n",
    "            if fallback_match:\n",
    "                cipher_texts.append(fallback_match[0][-1].strip())\n",
    "\n",
    "        if len(cipher_texts)>3:\n",
    "            cipher_texts=cipher_texts[:-1]\n",
    "        \n",
    "        return {}, cipher_texts[-1] if cipher_texts else generated_text.strip()\n",
    "\n",
    "def save_results(entry, json_file, is_last):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    json.dump(entry, json_file, indent=4)\n",
    "    json_file.write(\"\\n\" if is_last else \",\\n\")\n",
    "\n",
    "def write_config_line(experiment_id, total_lines, shift, type_of_plain_text, prompt_type, model, temperature, max_tokens):\n",
    "    \"\"\"Write configuration to config file\"\"\"\n",
    "    config_line = f\"{experiment_id}_{total_lines}_{shift}_{type_of_plain_text}_{prompt_type}_encrypt_0-{model}_{temperature}_{max_tokens}\\n\"\n",
    "    with open(CONFIG['PATHS']['CONFIG_FILE'], 'a') as config_file:\n",
    "        config_file.write(config_line)\n",
    "\n",
    "def process_model(model_path, lines_to_encode, shift, type_of_plain_text, model_index, total_models):\n",
    "    \"\"\"Process a single model\"\"\"\n",
    "    encoder = TextEncoder(model_path, CONFIG['EXPERIMENT']['TEMPERATURE'])\n",
    "    \n",
    "    for prompt_type, prompt_template in CONFIG['PROMPTS'].items():\n",
    "        max_tokens = CONFIG['GENERATION']['MAX_TOKENS'][prompt_type]\n",
    "        print(f\"\\nStarting encoding with model {encoder.model_name} using prompt type '{prompt_type}' (Model {model_index + 1} of {total_models}) with shift {shift}\")\n",
    "        \n",
    "        experiment_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file_path = os.path.join(CONFIG['PATHS']['OUTPUT_DIR'], f\"{experiment_id}.json\")\n",
    "        \n",
    "        write_config_line(experiment_id, len(lines_to_encode), shift, type_of_plain_text, \n",
    "                         prompt_type, encoder.model_name, encoder.temperature, max_tokens)\n",
    "        \n",
    "        with open(output_file_path, 'w') as json_file:\n",
    "            json_file.write(\"[\\n\")\n",
    "            \n",
    "            for line_idx, line in enumerate(tqdm(lines_to_encode, desc=f\"Encoding Progress with {encoder.model_name} ({prompt_type})\", dynamic_ncols=True)):\n",
    "                plain_text = line.strip()\n",
    "                if plain_text:\n",
    "                    lookup_table, cipher_text = encoder.encode(prompt_template, plain_text, shift, max_tokens, prompt_type)\n",
    "                    # print(f\"CIPHER_TEXT: {cipher_text}\")\n",
    "                    \n",
    "                    entry = {\n",
    "                        'plain_text': plain_text,\n",
    "                        'shift': shift,\n",
    "                        'cipher_text': cipher_text\n",
    "                    }\n",
    "                    if prompt_type == 'cot-like':\n",
    "                        entry['lookup_table'] = lookup_table\n",
    "                    \n",
    "                    save_results(entry, json_file, line_idx == len(lines_to_encode) - 1)\n",
    "            \n",
    "            json_file.write(\"]\\n\")\n",
    "        \n",
    "        print(f\"Finished encoding with {encoder.model_name} using prompt type '{prompt_type}' and shift {shift}. Results saved to {output_file_path}\\n\")\n",
    "    \n",
    "    # Clean up\n",
    "    del encoder.model\n",
    "    del encoder.tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Freed up GPU memory after processing {encoder.model_name}. Moving to the next model.\\n\")\n",
    "\n",
    "def main():\n",
    "    setup_environment()\n",
    "    \n",
    "    # Read input file\n",
    "    with open(CONFIG['PATHS']['INPUT_FILE'], 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    lines_to_encode = lines[:CONFIG['EXPERIMENT']['TOTAL_LINES']]\n",
    "    type_of_plain_text = os.path.splitext(os.path.basename(CONFIG['PATHS']['INPUT_FILE']))[0]\n",
    "    \n",
    "    # Process each shift value and model\n",
    "    for shift in CONFIG['EXPERIMENT']['SHIFT_VALUES']:\n",
    "        for model_index, model_path in enumerate(CONFIG['MODELS']):\n",
    "            process_model(model_path, lines_to_encode, shift, type_of_plain_text, \n",
    "                        model_index, len(CONFIG['MODELS']))\n",
    "    \n",
    "    print(\"All models, prompt types, and shift values have been processed. Exiting.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32363b08-99c1-4796-938d-c0de3804f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Vigenere Cipher on Natural Text\n",
    "# =======================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datetime import datetime\n",
    "\n",
    "CONFIG = {\n",
    "    'PATHS': {\n",
    "        'INPUT_FILE': './data/original/natural.txt',\n",
    "        'OUTPUT_DIR': './data/encoded/vigenere-cipher/',\n",
    "        'CONFIG_FILE': './vigenere-cipher-experiments-log.txt'\n",
    "    },\n",
    "    'MODELS': [\n",
    "        # '/storage/ukp/shared/shared_model_weights/models--llama-3/8B-Instruct/',\n",
    "        # '/storage/ukp/shared/shared_model_weights/models--llama-3/70B-Instruct/',\n",
    "        # '/storage/ukp/shared/shared_model_weights/models--mistralai--Mistral-7B-Instruct-v0.3/',\n",
    "        '/storage/ukp/shared/shared_model_weights/models--mistralai--Mixtral-8x7B-Instruct-v0.1/'\n",
    "    ],\n",
    "    'EXPERIMENT': {\n",
    "        'TOTAL_LINES': 25,\n",
    "        'KEY': 'Benchmark',\n",
    "        'TEMPERATURE': 0.01,\n",
    "        'SEED': 2266\n",
    "    },\n",
    "    'GENERATION': {\n",
    "        'MAX_TOKENS': {\n",
    "            'base': 64,\n",
    "            'cot-like': 512,\n",
    "            'default': 2024,\n",
    "        }\n",
    "    },\n",
    "    'PROMPTS': {\n",
    "        # 'base': \"Encode the following text using a Vigenère cipher with the key '$KEY$'. Output the cipher text only.\\n\\nplain text: \",\n",
    "        # 'cot-like': \"Encode the following text using a Vigenère cipher with the key '$KEY$'. Output a lookup table and the cipher text in a Python dictionary: {\\\"lookup_table\\\": {}, \\\"cipher_text\\\":...}. Output the dictionary only.\\n\\nplain text: \",\n",
    "        'default': \"Encode the following text using a Vigenère cipher with the key '$KEY$'.\\n\\nplain text: \"\n",
    "    },\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Patterns\n",
    "    # ------------------------------------------------------------------------\n",
    "    'PATTERNS': {\n",
    "        # Matches: \"lookup_table\": {...}\n",
    "        'LOOKUP_TABLE': r\"(?:\\\"lookup_table\\\":|\\'lookup_table\\':)\\s*(\\{[\\s\\S]*?\\})(?=,\\s*(?:\\\"cipher_text\\\":|\\'cipher_text\\':))\",\n",
    "      \n",
    "        'CIPHER_TEXT_DICT': r\"(?:\\\"cipher_text\\\":|\\'cipher_text\\':|`cipher_text`:)\\s*(\\\"[^\\\"]+\\\"|'[^']+'|`[^`]+`)\",\n",
    "  \n",
    "        'CIPHER_TEXT_ANSWER': r\"Answer:\\s*The cipher text is:\\s*([A-Za-z0-9]+)\",\n",
    "        \n",
    "        'CIPHER_TEXT_DIRECT': r\"(?:Cipher text:|Encrypted text:|Vigenère cipher:|Result:|Output:)\\s*([A-Za-z0-9]+)\",\n",
    "\n",
    "        'CIPHER_TEXT_QUOTED': r\"[\\\"'`](?!Benchmark)([A-Za-z0-9]+)[\\\"'`]\",\n",
    "        # Matches if the cipher text is on its own line between newlines:\n",
    "        'CIPHER_TEXT_NEWLINE': r\"\\n([A-Za-z0-9]+)(?:\\n|$)\",\n",
    "\n",
    "        'GENERAL_OUTPUT': r\"(?:The encoded text is:|The encryption result is:|The Vigenère cipher text is:|Here's the encrypted text:|The result is:)\\s*([A-Za-z0-9]+)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class TextEncoder:\n",
    "    def __init__(self, model_path, temperature=0.01):\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.temperature = temperature\n",
    "        self.model_name = model_path.split('/')[-2]\n",
    "\n",
    "    def encode(self, prompt_template, text, key, max_tokens, prompt_type):\n",
    "        prompt = prompt_template.replace(\"$KEY$\", key) + text\n",
    "        print(f\"\\nPrompt:\\n{prompt}\\n\")\n",
    "        return self.generate_text(prompt, max_tokens, prompt_type)\n",
    "\n",
    "    def generate_text(self, prompt, max_tokens, prompt_type):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        generated_text = \"\\n\".join(line for line in generated_text.splitlines() if line.strip())\n",
    "        print(\"***************************************\")\n",
    "        print(f\"Generated text:\\n{generated_text}\\n\")\n",
    "        print(\"***************************************\")\n",
    "        return self._process_generated_text(generated_text, prompt_type)\n",
    "    \n",
    "    def _process_generated_text(self, generated_text, prompt_type):\n",
    "        if prompt_type == 'cot-like':\n",
    "            return self._process_cot_output(generated_text)\n",
    "        return self._process_regular_output(generated_text)\n",
    "    \n",
    "    def _process_cot_output(self, generated_text):\n",
    "        # Try to extract lookup table\n",
    "        lookup_match = re.search(CONFIG['PATTERNS']['LOOKUP_TABLE'], generated_text, flags=re.IGNORECASE)\n",
    "        lookup_table = {}\n",
    "        if lookup_match:\n",
    "            lookup_str = lookup_match.group(1)\n",
    "            try:\n",
    "                # Attempt to parse the lookup table using ast.literal_eval\n",
    "                lookup_table = ast.literal_eval(lookup_str)\n",
    "                # Ensure that the 'lookup_table' key exists\n",
    "                if 'lookup_table' in lookup_table:\n",
    "                    lookup_table = lookup_table['lookup_table']\n",
    "                else:\n",
    "                    lookup_table = {}\n",
    "            except (SyntaxError, ValueError, KeyError) as e:\n",
    "                print(f\"Warning: Could not parse lookup table. Error: {e}\")\n",
    "                lookup_table = {}\n",
    "        else:\n",
    "            print(\"Warning: No lookup table pattern matched.\")\n",
    "        \n",
    "        # Try to extract cipher text from dictionary format\n",
    "        cipher_text = \"\"\n",
    "        dict_match = re.search(CONFIG['PATTERNS']['CIPHER_TEXT_DICT'], generated_text, flags=re.IGNORECASE)\n",
    "        if dict_match:\n",
    "            cipher_text = dict_match.group(1).strip('\"\\'`')\n",
    "        else:\n",
    "            # Fallback to the broader pattern matching\n",
    "            cipher_text = self._extract_cipher_text(generated_text)\n",
    "        \n",
    "        print(f\"Processed COT output:\\nLookup table: {lookup_table}\\nCipher text: {cipher_text}\\n\")\n",
    "        return lookup_table, cipher_text\n",
    "    \n",
    "    def _process_regular_output(self, generated_text):\n",
    "        cipher_text = self._extract_cipher_text(generated_text)\n",
    "        print(f\"Processed regular output:\\nCipher text: {cipher_text}\\n\")\n",
    "        return {}, cipher_text\n",
    "    \n",
    "    def _extract_cipher_text(self, text):\n",
    "        \"\"\"\n",
    "        Attempt multiple regex patterns in sequence with case-insensitive matching.\n",
    "        As soon as one matches, return the captured group.\n",
    "        If none match, return the cleaned version of the entire text as a last resort.\n",
    "        \"\"\"\n",
    "        patterns = [\n",
    "            CONFIG['PATTERNS']['CIPHER_TEXT_ANSWER'],      # New pattern\n",
    "            CONFIG['PATTERNS']['CIPHER_TEXT_DIRECT'],\n",
    "            CONFIG['PATTERNS']['CIPHER_TEXT_QUOTED'],\n",
    "            CONFIG['PATTERNS']['CIPHER_TEXT_NEWLINE'],\n",
    "            CONFIG['PATTERNS']['GENERAL_OUTPUT']\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                captured = match.group(1).strip()\n",
    "                print(f\"Matched pattern: {pattern}\\nCaptured cipher text: {captured}\")\n",
    "                return captured\n",
    "        \n",
    "        # If no pattern matches, return the cleaned text as a last resort\n",
    "        cleaned_text = ' '.join(text.split()).strip()\n",
    "        print(f\"Warning: No pattern matched. Using cleaned text: {cleaned_text}\")\n",
    "        return cleaned_text\n",
    "\n",
    "def process_model(model_path, lines_to_encode):\n",
    "    encoder = TextEncoder(model_path, CONFIG['EXPERIMENT']['TEMPERATURE'])\n",
    "    \n",
    "    for prompt_type, prompt_template in CONFIG['PROMPTS'].items():\n",
    "        experiment_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = os.path.join(CONFIG['PATHS']['OUTPUT_DIR'], f\"{experiment_id}.json\")\n",
    "        os.makedirs(CONFIG['PATHS']['OUTPUT_DIR'], exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {encoder.model_name} with prompt type '{prompt_type}'\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        with open(CONFIG['PATHS']['CONFIG_FILE'], 'a') as config_file:\n",
    "            config_file.write(f\"{experiment_id}_{len(lines_to_encode)}_{CONFIG['EXPERIMENT']['KEY']}_{prompt_type}_{encoder.model_name}\\n\")\n",
    "        \n",
    "        with open(output_file, 'w') as json_file:\n",
    "            json_file.write(\"[\\n\")\n",
    "            \n",
    "            for i, line in enumerate(tqdm(lines_to_encode)):\n",
    "                if line.strip():\n",
    "                    print(f\"\\nProcessing line {i+1}/{len(lines_to_encode)}:\")\n",
    "                    print(f\"Input text: {line.strip()}\")\n",
    "                    \n",
    "                    lookup_table, cipher_text = encoder.encode(\n",
    "                        prompt_template, \n",
    "                        line.strip(), \n",
    "                        CONFIG['EXPERIMENT']['KEY'],\n",
    "                        CONFIG['GENERATION']['MAX_TOKENS'][prompt_type],\n",
    "                        prompt_type\n",
    "                    )\n",
    "                    \n",
    "                    result = {\n",
    "                        'plain_text': line.strip(),\n",
    "                        'key': CONFIG['EXPERIMENT']['KEY'],\n",
    "                        'cipher_text': cipher_text,\n",
    "                    }\n",
    "                    if prompt_type == 'cot-like':\n",
    "                        result['lookup_table'] = lookup_table\n",
    "                    \n",
    "                    json.dump(result, json_file, indent=4)\n",
    "                    json_file.write(\",\\n\" if i < len(lines_to_encode) - 1 else \"\\n\")\n",
    "                    \n",
    "                    print(f\"Result: {json.dumps(result, indent=2)}\\n\")\n",
    "            \n",
    "            json_file.write(\"]\\n\")\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    del encoder.model, encoder.tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    torch.manual_seed(CONFIG['EXPERIMENT']['SEED'])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(CONFIG['PATHS']['OUTPUT_DIR'], exist_ok=True)\n",
    "    \n",
    "    print(\"\\nStarting Vigenère cipher encoding process...\")\n",
    "    print(f\"Using key: {CONFIG['EXPERIMENT']['KEY']}\")\n",
    "    print(f\"Input file: {CONFIG['PATHS']['INPUT_FILE']}\")\n",
    "    print(f\"Output directory: {CONFIG['PATHS']['OUTPUT_DIR']}\\n\")\n",
    "    \n",
    "    # Read input file\n",
    "    with open(CONFIG['PATHS']['INPUT_FILE'], 'r') as file:\n",
    "        lines = file.readlines()[:CONFIG['EXPERIMENT']['TOTAL_LINES']]\n",
    "    \n",
    "    print(f\"Loaded {len(lines)} lines for processing\\n\")\n",
    "    \n",
    "    # Process each model\n",
    "    for model_path in CONFIG['MODELS']:\n",
    "        print(f\"\\nProcessing model: {model_path.split('/')[-2]}\")\n",
    "        process_model(model_path, lines)\n",
    "    \n",
    "    print(\"\\nAll processing completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800b74b-3586-4ec8-a397-087e6b66cd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
